{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banana Navigation\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  \n",
    "Before start please make use to properly installed requirement from README.md - Installation requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from Agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change BANANA_INSTALLATION to your installation path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_MODE = True\n",
    "EPISODES_NUM = 1750\n",
    "BANANA_INSTALLATION = \"../proj_banan/Banana_Windows_x86_64/Banana.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create the class of Experience Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExperienceManager:\n",
    "\n",
    "    def __init__(self):\n",
    "        #define the params for later usage\n",
    "        self.env = None\n",
    "        self.brain_name = None\n",
    "        self.agent = None\n",
    "        \n",
    "    #initialize enviroment and set the state space size and  action space size\n",
    "    def initEnviroment(self):\n",
    "        print('Initialize env')\n",
    "        #initalize Unity env\n",
    "        self.env = UnityEnvironment(file_name=BANANA_INSTALLATION)\n",
    "        #get the default brain\n",
    "        self.brain_name = self.env.brain_names[0]\n",
    "        #reset the environment\n",
    "        env_info = self.env.reset(train_mode=TRAIN_MODE)[self.brain_name]\n",
    "        #get size of action and state\n",
    "        self.action_size = self.env.brains[self.brain_name].vector_action_space_size\n",
    "        self.state_size = len(env_info.vector_observations[0])\n",
    "        #initiate Agent\n",
    "        self.agent = Agent(state_size=self.state_size, action_size=self.action_size, seed=0)\n",
    "        print('Env init done')\n",
    "\n",
    "    #run one episode and return total reward\n",
    "    def runEpisode(self):\n",
    "       #init score to 0 and reset env\n",
    "       score = 0\n",
    "       state = self.env.reset(train_mode=TRAIN_MODE)[self.brain_name].vector_observations[0]\n",
    "       while True:\n",
    "            #get greedy action\n",
    "            action = self.agent.greedy_action(state)\n",
    "            #perform action\n",
    "            env_info = self.env.step(action)[self.brain_name]\n",
    "            #get the step result\n",
    "            next_state = env_info.vector_observations[0]  # get the next state\n",
    "            reward = env_info.rewards[0]  # get the reward\n",
    "            done = env_info.local_done[0]  # see if episode has finished\n",
    "            #store step result and perform learning\n",
    "            self.agent.step(state, action, reward, next_state, done)\n",
    "            #update state and score\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                #finito\n",
    "                break\n",
    "       #return the score of whole episode\n",
    "       return score\n",
    "\n",
    "     #run the whole experiments = defined number of episodes\n",
    "    def runEperiment(self,n_episodes=EPISODES_NUM):\n",
    "        #init enviroment\n",
    "        self.initEnviroment()\n",
    "        scores = []       \n",
    "        scores_window = deque(maxlen=100)  # last 100 scores\n",
    "        for i_episode in range(1, n_episodes + 1):\n",
    "            #run one episode\n",
    "            score =  self.runEpisode()\n",
    "            #store the score of episode\n",
    "            scores_window.append(score)\n",
    "            scores.append(score)\n",
    "            #print progress\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            #keep progress of last 100 episodes\n",
    "            if i_episode % 100 == 0:\n",
    "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))           \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions that store what the agent learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#store the trained DQN pytorch network\n",
    "def store_trained_network(exp_manager,filename):\n",
    "    torch.save(exp_manager.agent.qnetwork_local.state_dict(),filename )\n",
    "\n",
    "#save the progress of success\n",
    "def save_plot_scores(scores,filename):\n",
    "    # plot the scores\n",
    "    fig = plt.figure()\n",
    "    fig.add_subplot(111)\n",
    "    plt.plot(np.arange(len(scores)), scores)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    #save the plot to file\n",
    "    plt.savefig(filename + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment whatch how the agent learn and save the result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Starts....\n",
      "Initialize env\n"
     ]
    }
   ],
   "source": [
    "print('Experiment Starts....')\n",
    "#Initialize Manager\n",
    "exp_man = ExperienceManager()\n",
    "#run experiment\n",
    "scores = exp_man.runEperiment()\n",
    "print('Experiment finish')\n",
    "#get date for storing results\n",
    "date_sufix = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\n",
    "#store trained network\n",
    "store_trained_network(exp_man,'banana_model_'+date_sufix+'.pth')\n",
    "#store plot of score\n",
    "save_plot_scores(scores,'score_'+date_sufix+'.png')\n",
    "print('Store results done')\n",
    "print('Job done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
